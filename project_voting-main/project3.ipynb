{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20673812-4dc7-4b66-9dc8-b4b9be6876cf",
   "metadata": {},
   "source": [
    "# Programming Project #3\n",
    "## Foundations of Machine Learning\n",
    "\n",
    "The purpose of this project is to investigate gerrymandering in Virginia and predictive algorithms for that purpose.\n",
    "  \n",
    "The data include:\n",
    "  \n",
    "  - `voting_VA.csv`: Voting data for presidential elections for Virginia from 2000 to 2020\n",
    "  - `nhgis_county_data`: A folder containing many county-level summary stats for every county in the U.S. This is the most complete county-level data I could find. If you go to the IPUMS NHGIS web site, you can see what else is available (there are hundreds of variables, and I chose a large number of obvious ones; perhaps some useful ones escaped my attention). For standard IPUMS microdata, the county is not available for privacy reasons.\n",
    "  - `county_adjacencies.csv`: I looked up the neighbors, districts, FIPS county identifiers, and populations in 2022 for all counties and cities in Virginia.\n",
    "  - At [https://vgin.vdem.virginia.gov/datasets/777890ecdb634d18a02eec604db522c6/about] there is a shapefile for making choropleth plots called \"Shapefile Download (Clipped to VIMS shoreline)\"\n",
    "  -  I put together a starter notebook called `va_voting.ipynb` that shows how to combine these files and making nice maps\n",
    "\n",
    "There are 134 counties in Virginia that must be allocated into 11 voting districts. In reality, voting districts do not need to respect county boundaries, but this simplifying assumption makes the project much more manageable. \n",
    "  \n",
    "You can use whatever data you want to create a predictive algorithm for voting outcomes, based on the `voting_VA.csv` and `nhgis_county_data` data or other sources you think would be useful. You can focus on Virginia data, but in principle, you could use data from the entire country. Since you only have five observations for each county on its own in Virginia, you can, in principle, use the additional data about county composition or data from other states to build richer and more powerful predictive models than just the sample average for each county (e.g 3 observations of `D` winning and 2 of `R` winning implies a probability 3/5 of `D` winning). You could also gather and use data about past candidates to see if there are county-candidate interaction effects that improve your model's performance. Indeed, 2024 might be a Biden-Trump rematch, in which case past data might be extremely relevant.\n",
    "\n",
    "You will describe a method for gerrymandering, but not necessarily implement it. That would be a lot of work and computational burden. We want to focus on how you would then build a predictive algorithm for your method.\n",
    " \n",
    "The rules for gerrymandering are fairly complex in the real world, but we will keep it simple:\n",
    "  \n",
    "  1. Voting districts must be *contiguous*, so that you can travel from any point to any other point within a voting district without leaving the voting district. For us, the counties must be adjacent, so that their borders touch.\n",
    "  2. Voting districts must be *proportional*, so they are roughly of equal size in terms of population. We'll try to keep it simple: Your largest voting district cannot be more than 5% larger in terms of population than your smallest voting district. (Currently, the eighth district has a population of 770k and the first has a population of 805k, so 4.55% larger.)\n",
    "  \n",
    "This kind of problem is difficult to solve (it is similar to a \"bin packing\" problem, known to computationally difficult; i.e. NP-Hard). I would not use a direct approach (e.g. linear programming), but would instead consider starting with an \"electoral map\" similar to what currently exists and then writing a random algorithm that looks to flip counties in ways that don't violate the constraints and do improve the predicted outcomes for the `R` and `D` parties. Or, I might start by finding the 11 largest counties that are most supportive of the party I'm optimizing for, and then expanding from those, adding additional adjacent counties to each district that maintain proportionality and contiguity but minimize the number of districts that the other party wins.\n",
    "    \n",
    "## Paper format\n",
    "\n",
    "The format of the paper should be:\n",
    "\n",
    "  - Summary: A one paragraph description of the question, methods, and results (about 350 words).\n",
    "  - Data: One to two pages discussing the data and key variables, and any challenges in reading, cleaning, and preparing them for analysis.\n",
    "  - Results: Two to five pages providing visualizations, statistics, tables, a discussion of your methodology, and a presentation of your main results. In particular, how are you approaching the prediction and optimization problems? How confident are you about your assessments that counties will support one party or the other? Which party seems to have an advantage in terms of drawing the electoral map?\n",
    "  - Conclusion: One to two pages summarizing the project, defending it from criticism, and suggesting additional work that was outside the scope of the project.\n",
    "  - Appendix: If you have a significant number of additional plots or table that you feel are essential to the project, you can put any amount of extra content at the end and reference it from the body of the paper.\n",
    "\n",
    "Submit your work in your group's GitHub repo.\n",
    "\n",
    "## Criteria\n",
    "\n",
    "The project is graded based on four criteria:\n",
    "\n",
    "  - Project Concept: What is the strategy for building and testing the group's predictive models? How are the models embedded in the decision problem of gerrymandering Virginia?\n",
    "  \n",
    "  - Wrangling, EDA, and Visualization: How are are missing values handled? For variables with large numbers of missing values, to what extent do the data and documentation provide an explanation for the missing data? If multiple data sources are used, how are the data merged? For the main variables in the analysis, are the relevant data summarized and visualized through a histogram or kernel density plot where appropriate? Are basic quantitative features of the data addressed and explained? How are outliers characterized and addressed? \n",
    "  - Analysis: What are the groups' main findings? Do the tables, plots, and statistics support the conclusions? If the gerrymandering strategy succeeds, what are the results and how extreme can the map be drawn for each side? If the gerrymandering strategy fails, is there a thoughtful discussion about the challenges and limitations?\n",
    "  \n",
    "  - Replication/Documentation: Is the code appropriately commented? Can the main results be replicated from the code and original data files? Are significant choices noted and explained?\n",
    "\n",
    "Each of the four criteria are equally weighted (25 points out of 100).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
